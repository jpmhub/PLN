{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "40ee686e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# EN AQUESTS EXEMPLES TREBALLAREM AMB UNA CARACTERÍSTICA BÀSICA PER ANALITZAR UN TEXT\n",
    "# ANALITZAREM EL NOMBRE DE VEGADES QUE APAREIX CADA TERME AMB L'OBJECTE CountVectorizer\n",
    "import nltk\n",
    "nltk.download(\"stopwords\") \n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "#sklearn: llibreria per aprenentatge automàtic (Machine Learning) de Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "05eebb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_1=\"Este ha sido un buen trabajo. No dudaría dudaría en volver a repetirlo\"\n",
    "doc_2=\"Esto no ha sido para nada un buen trabajo realizado\"\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "# creem un nou objecte vectorizer al qual li passem les stopwords del castellà\n",
    "vectorizer = CountVectorizer(stop_words = spanish_stopwords)\n",
    "# CountVectorizer és un objecte que permet extreure les paraules de cada document\n",
    "# fixa't que llevam de la colecció les stopwords!! això és opcional però recomanable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "388cf44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulari:  {'sido': 4, 'buen': 0, 'trabajo': 5, 'dudaría': 1, 'volver': 6, 'repetirlo': 3, 'realizado': 2}\n"
     ]
    }
   ],
   "source": [
    "# entrenarem amb tota la colecció per extreure el vocabulari\n",
    "# es pot treballar només amb una part de la colecció o bé amb tota\n",
    "vectorizer.fit([doc_1,doc_2])  \n",
    "print(\"Vocabulari: \", vectorizer.vocabulary_)# imprimeix les paraules detectades\n",
    "# el nº que surt devora cada paraula és l'índex del vector.\n",
    "# tot el vocabulari es passa a minúscules i per defecte ignora els signes de puntuació"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "954a1240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulari de la nostra colecció: \n",
      " ['buen', 'dudaría', 'realizado', 'repetirlo', 'sido', 'trabajo', 'volver']\n",
      "Nº de vegades que apareix cada paraula de la colecció:  [[1 0 0 1 1 0 0]]\n",
      "Files i columnes de la matriu resultant: (1, 7)\n"
     ]
    }
   ],
   "source": [
    "# el mètode transform converteix una colecció de documents en un vector de paraules\n",
    "doc_3=\"Este ha sido un buen año pero no volvería a repetirlo\"\n",
    "dades = vectorizer.transform([doc_3])\n",
    "print(\"Vocabulari de la nostra colecció: \\n\", vectorizer.get_feature_names())# imprimeix les paraules detectades\n",
    "# si aquest vector té 7 posicions el següent vector que compta la greqüència de cada paraula també tendrà 7 posicions.\n",
    "print(\"Nº de vegades que apareix cada paraula de la colecció: \", dades.toarray())# \"toarray\" mostra el nombre de vegades que apareix cada terme.\n",
    "print(\"Files i columnes de la matriu resultant:\", dades.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ae90f990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulari de la nostra colecció: \n",
      " ['buen', 'dudaría', 'realizado', 'repetirlo', 'sido', 'trabajo', 'volver']\n",
      "Nº de vegades que apareix cada paraula de la colecció:  [[0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# si ara el text d'entrada no té cap terme que estigui a les dades d'entrenament els ignorarà!!\n",
    "doc_4=\"Si lloviera mañana podría crecer la hierba.\"\n",
    "print(\"Vocabulari de la nostra colecció: \\n\", vectorizer.get_feature_names())\n",
    "dades = vectorizer.transform([doc_4])\n",
    "print(\"Nº de vegades que apareix cada paraula de la colecció: \", dades.toarray())\n",
    "# tot són \"0\" perquè cap de les paraules que hi ha a la nostra colecció apareix a la frase doc_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5c370a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   buen  dudaría  realizado  repetirlo  sido  trabajo  volver\n",
      "0     1        2          0          1     1        1       1\n",
      "1     1        0          1          0     1        1       0\n"
     ]
    }
   ],
   "source": [
    "# creem dataframe per a visualitzar millor les dades.\n",
    "# en aquest cas la matriu resultant també ens diu quantes vegades apareix cada terme a cada frase.\n",
    "import pandas as pd\n",
    "dades = vectorizer.transform([doc_1,doc_2])\n",
    "# les files indicaran la freqüència i les columnes les paraules de la nostra colecció\n",
    "matriu=pd.DataFrame(dades.toarray(), columns=vectorizer.get_feature_names())\n",
    "print(matriu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "50643d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Com filtrar les paraules de la nostra colecció???? Aquí ho tenim!!\n",
    "# min_df: ignora les paraules que apareixen menys del 30% en els documents\n",
    "# max_df: ignora les paraules que apareixen més del 50% en els documents\n",
    "# min_df < max_df\n",
    "# max_features: treballarà amb les 50 paraules amb més aparicions en els documents\n",
    "vectorizer = CountVectorizer(min_df = 0.3, max_df = 0.5, max_features = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8ba47ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords detectades:  {'un', 'no', 'ha', 'trabajo', 'sido', 'buen'}\n",
      "   dudaría  en  este  esto  nada  para  realizado  repetirlo  volver\n",
      "0        2   1     1     0     0     0          0          1       1\n",
      "1        0   0     0     1     1     1          1          0       0\n"
     ]
    }
   ],
   "source": [
    "# El mètode fit_transform() permet entrenar i analitzar a la vegada si es fa sobre la mateixa col·lecció\n",
    "dades = vectorizer.fit_transform([doc_1,doc_2])\n",
    "print(\"Stopwords detectades: \", vectorizer.stop_words_)\n",
    "# definim la matriu amb les files (documents) i columnes (paraules)\n",
    "# cada posició de la matriu mostra el nº de vegades que surt cada terme\n",
    "matriu=pd.DataFrame(dades.toarray(),columns=vectorizer.get_feature_names())\n",
    "print(matriu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7b58d9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stop-words in c:\\users\\anton\\anaconda3\\lib\\site-packages (2018.7.23)\n",
      "Stopwords catalanes: \n",
      " {'ans', '', 'últim', 'tot', 'tenim', 'alguns', 'som', 'amb', 'sota', 'poden', 'ells', 'fer', 'te', 'haver', 'consigueix', 'soc', 'fem', 'inclòs', 'les', 'per', 'altre', 'cada', 'conseguir', 'aquells', 'si', 'teu', 'estava', 'es', 'conseguim', 'aquell', '\\ufeffa', 'estan', 'qui', 'en', 'primer', 'sabeu', 'estem', 'feu', 'podem', 'fan', 'mode', 'tinc', 'una', 'esteu', 'bé', 'sabem', 'tene', 'ens', 'vaig', 'anar', 'éssent', 'des de', 'elles', 'fas', 'mentre', 'com', 'teniu', 'vosaltres', 'algunes', 'els', 'dins', 'poder', 'consegueixo', 'també', 'algun', 'on', 'era', 'aquelles', 'sap', 'de', 'llavors', 'meu', 'van', 'estic', 'solament', 'faig', 'saber', 'unes', 'consigueixen', 'fa', 'puc', 'aquí', 'o', 'molt', 'perquè', 'ambdós', 'saben', 'estat', 'seu', 'ets', 'alguna', 'per que', 'i', 'bastant', 'ser', 'el', 'potser', 'eren', 'entre', 'llarg', 'consigueixes', 'sols', 'tenir', 'però', 'la', 'un', 'molts', 'podeu', 'sense', 'està', 'va', 'fi', 'jo', 'abans', 'erem', 'seus', 'dalt', 'quan', 'saps', 'quant', 'eres', 'és', 'nosaltres', 'ús', 'uns'}\n",
      "\n",
      " \n",
      " La matriu resultat és: \n",
      " \n",
      "    aquesta  casa  gran  llicència  lloguer  massa  necessites  vendre  vull\n",
      "0        0     1     1          0        0      1           0       1     1\n",
      "1        1     1     0          1        1      0           1       1     0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['des', 'que'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "!pip install stop-words\n",
    "from stop_words import get_stop_words\n",
    "catala_stopwords = set(get_stop_words('ca'))\n",
    "print(\"Stopwords catalanes: \\n\",catala_stopwords )\n",
    "doc_5=\"La casa era massa gran per jo, la vull vendre\"\n",
    "doc_6=\"Per vendre aquesta casa necessites llicència de lloguer\"\n",
    "vectorizer1 = CountVectorizer(stop_words = catala_stopwords)\n",
    "dades_catala = vectorizer1.fit_transform([doc_5,doc_6])\n",
    "# les files indicaran la freqüència i les columnes les paraules de la nostra colecció\n",
    "matriu_catala=pd.DataFrame(dades_catala.toarray(), columns=vectorizer1.get_feature_names())\n",
    "print(\"\\n \\n La matriu resultat és: \\n \\n\", matriu_catala)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19521f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
