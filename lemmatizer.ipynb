{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8b6f954",
   "metadata": {},
   "source": [
    "Juan Pablo Márquez 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc5f592a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\anton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\anton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\anton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\anton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\anton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\anton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\anton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\anton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\anton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\anton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\anton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\anton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\anton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\anton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\anton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\anton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\anton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\anton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\anton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\anton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\anton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\anton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"popular\")\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34e07d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amaze\n",
      "amazing\n"
     ]
    }
   ],
   "source": [
    "# exemple extreure el lema de la paraula \"amazing\".\n",
    "# el paràmetre 'pos' ens ajuda perquè les formes podrien ser iguals però contextualment o semànticament diferents.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"amazing\", pos ='v')) # amazing com a verb\n",
    "print(lemmatizer.lemmatize(\"amazing\", pos ='n')) # amazing com a nom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "431e7e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pentagon', 'identifies', '13', 'amazing', 'service', 'members', 'killed', 'in', 'terrorist', 'attack', 'at', 'Kabul', 'airports']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Pentagon',\n",
       " 'identifies',\n",
       " '13',\n",
       " 'amazing',\n",
       " 'service',\n",
       " 'member',\n",
       " 'killed',\n",
       " 'in',\n",
       " 'terrorist',\n",
       " 'attack',\n",
       " 'at',\n",
       " 'Kabul',\n",
       " 'airport']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#exemple de com extreure els lemes a partir de una frase/text\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sent = 'Pentagon identifies 13 amazing service members killed in terrorist attack at Kabul airports'\n",
    "tokens = sent.split(\" \")\n",
    "print (tokens)\n",
    "#[lemmatizer.lemmatize(t) for t in tokens]\n",
    "#[lemmatizer.lemmatize(t, pos = 'v') for t in tokens]\n",
    "[lemmatizer.lemmatize(t, pos = 'n') for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e4c596c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stanza in c:\\users\\anton\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: requests in c:\\users\\anton\\anaconda3\\lib\\site-packages (from stanza) (2.26.0)\n",
      "Requirement already satisfied: emoji in c:\\users\\anton\\anaconda3\\lib\\site-packages (from stanza) (1.6.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\anton\\anaconda3\\lib\\site-packages (from stanza) (1.20.3)\n",
      "Requirement already satisfied: torch>=1.3.0 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from stanza) (1.10.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\anton\\anaconda3\\lib\\site-packages (from stanza) (4.62.3)\n",
      "Requirement already satisfied: six in c:\\users\\anton\\anaconda3\\lib\\site-packages (from stanza) (1.16.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\anton\\anaconda3\\lib\\site-packages (from stanza) (3.19.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\anton\\anaconda3\\lib\\site-packages (from torch>=1.3.0->stanza) (3.10.0.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from requests->stanza) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from requests->stanza) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from requests->stanza) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\anton\\anaconda3\\lib\\site-packages (from requests->stanza) (2021.10.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\anton\\anaconda3\\lib\\site-packages (from tqdm->stanza) (0.4.4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b5630a3502c495eb4691774c49df6ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.3.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-12 11:13:46 INFO: Downloading these customized packages for language: es (Spanish)...\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ancora  |\n",
      "| mwt       | ancora  |\n",
      "| pos       | ancora  |\n",
      "| lemma     | ancora  |\n",
      "| pretrain  | ancora  |\n",
      "=======================\n",
      "\n",
      "2022-01-12 11:13:46 INFO: File exists: C:\\Users\\anton\\stanza_resources\\es\\tokenize\\ancora.pt.\n",
      "2022-01-12 11:13:46 INFO: File exists: C:\\Users\\anton\\stanza_resources\\es\\mwt\\ancora.pt.\n",
      "2022-01-12 11:13:46 INFO: File exists: C:\\Users\\anton\\stanza_resources\\es\\pos\\ancora.pt.\n",
      "2022-01-12 11:13:46 INFO: File exists: C:\\Users\\anton\\stanza_resources\\es\\lemma\\ancora.pt.\n",
      "2022-01-12 11:13:46 INFO: File exists: C:\\Users\\anton\\stanza_resources\\es\\pretrain\\ancora.pt.\n",
      "2022-01-12 11:13:46 INFO: Finished downloading models and saved to C:\\Users\\anton\\stanza_resources.\n",
      "2022-01-12 11:13:46 INFO: Loading these models for language: es (Spanish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ancora  |\n",
      "| mwt       | ancora  |\n",
      "| pos       | ancora  |\n",
      "| lemma     | ancora  |\n",
      "=======================\n",
      "\n",
      "2022-01-12 11:13:46 INFO: Use device: cpu\n",
      "2022-01-12 11:13:46 INFO: Loading: tokenize\n",
      "2022-01-12 11:13:46 INFO: Loading: mwt\n",
      "2022-01-12 11:13:46 INFO: Loading: pos\n",
      "2022-01-12 11:13:46 INFO: Loading: lemma\n",
      "2022-01-12 11:13:46 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: Pedro \tlemma: Pedro \tpos: PROPN\n",
      "word: fue \tlemma: ser \tpos: AUX\n",
      "word: el \tlemma: el \tpos: DET\n",
      "word: primero \tlemma: primero \tpos: ADJ\n",
      "word: en \tlemma: en \tpos: ADP\n",
      "word: vistar \tlemma: vistar \tpos: VERB\n",
      "word: Rusia \tlemma: Rusia \tpos: PROPN\n",
      "word: . \tlemma: . \tpos: PUNCT\n",
      "word: Compró \tlemma: comprar \tpos: VERB\n",
      "word: manzanas \tlemma: manzana \tpos: NOUN\n",
      "word: y \tlemma: y \tpos: CCONJ\n",
      "word: huevos \tlemma: huevo \tpos: NOUN\n",
      "word: duros \tlemma: duro \tpos: ADJ\n",
      "word: . \tlemma: . \tpos: PUNCT\n"
     ]
    }
   ],
   "source": [
    "# 'Wordet' està pensat per l'anglès. Si volem treballar en castellà podem emprar la llibreria 'stanza'\n",
    "# concretament emprarem el diccionari 'ancora' de 'stanza'\n",
    "!pip install stanza \n",
    "import stanza\n",
    "# el paràmetre 'processors' ens serveix per indicar la informació que voldrem extreure (tokenize,mwt,pos,lemma)\n",
    "# tokenize=,mwt=ens indica si la paraula és singular o plural, gènere, majúscules, etc. .,pos=POS,lemma=lema\n",
    "stanza.download('es', package='ancora', processors='tokenize,mwt,pos,lemma', verbose=True)\n",
    "# definim una tuberia 'Pipeline' dins la qual posem tots el mètodes que volem emprar: tokenitzar, etiquetar, lematitzar\n",
    "stNLP = stanza.Pipeline(processors='tokenize,mwt,pos,lemma', lang='es', use_gpu=True) \n",
    "doc = stNLP('Pedro fue el primero en vistar Rusia. Compró manzanas y huevos duros.') \n",
    "\n",
    "# aquest form recorr totes les frases de la variable doc i per cada una d'elles imprimeix la paraula, el lema i el POS\n",
    "print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma} \\tpos: {word.pos}' \n",
    "        for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be08d26e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27f661d1e2e34dd3aab68c80c5459b83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.3.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-12 11:15:24 INFO: Downloading default packages for language: en (English)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e559af114c848858f7839e93647b8b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.3.0/models/default.zip:   0%|          | 0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-12 11:16:34 INFO: Finished downloading models and saved to C:\\Users\\anton\\stanza_resources.\n",
      "2022-01-12 11:16:34 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
      "2022-01-12 11:16:34 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "========================\n",
      "\n",
      "2022-01-12 11:16:34 INFO: Use device: cpu\n",
      "2022-01-12 11:16:34 INFO: Loading: tokenize\n",
      "2022-01-12 11:16:34 INFO: Loading: pos\n",
      "2022-01-12 11:16:34 INFO: Loading: lemma\n",
      "2022-01-12 11:16:34 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: Pentagon \tlemma: Pentagon \tpos: PROPN\n",
      "word: identifies \tlemma: identify \tpos: VERB\n",
      "word: 13 \tlemma: 13 \tpos: NUM\n",
      "word: amazing \tlemma: amazing \tpos: ADJ\n",
      "word: service \tlemma: service \tpos: NOUN\n",
      "word: members \tlemma: member \tpos: NOUN\n",
      "word: killed \tlemma: kill \tpos: VERB\n",
      "word: in \tlemma: in \tpos: ADP\n",
      "word: terrorist \tlemma: terrorist \tpos: ADJ\n",
      "word: attack \tlemma: attack \tpos: NOUN\n",
      "word: at \tlemma: at \tpos: ADP\n",
      "word: Kabul \tlemma: Kabul \tpos: PROPN\n",
      "word: airports \tlemma: airport \tpos: NOUN\n"
     ]
    }
   ],
   "source": [
    "# fem el mateix però ara descarregant el paquest d'anglès.\n",
    "import stanza \n",
    "stanza.download('en') \n",
    "stNLP = stanza.Pipeline(processors='tokenize,mwt,pos,lemma', lang='en', use_gpu=True) \n",
    "doc = stNLP('Pentagon identifies 13 amazing service members killed in terrorist attack at Kabul airports') \n",
    "print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma} \\tpos: {word.pos}' \n",
    "        for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "520e3ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase a analitzar: Setze jutges d'un jutjat menjaven fetges d'un penjat.\n",
      "\n",
      "paraula: Setze\t POS: NUM\t lemma: Setze\n",
      "\n",
      "paraula: jutges\t POS: NOUN\t lemma: jutge\n",
      "\n",
      "paraula: d'\t POS: ADP\t lemma: de\n",
      "\n",
      "paraula: un\t POS: DET\t lemma: un\n",
      "\n",
      "paraula: jutjat\t POS: NOUN\t lemma: jutjat\n",
      "\n",
      "paraula: menjaven\t POS: VERB\t lemma: menjar\n",
      "\n",
      "paraula: fetges\t POS: NOUN\t lemma: fetge\n",
      "\n",
      "paraula: d'\t POS: ADP\t lemma: de\n",
      "\n",
      "paraula: un\t POS: DET\t lemma: un\n",
      "\n",
      "paraula: penjat\t POS: NOUN\t lemma: penjat\n",
      "\n",
      "paraula: .\t POS: PUNCT\t lemma: .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pel català una bona llibreria podria ser spacy\n",
    "import spacy\n",
    "from spacy.lang.ca.examples import sentences\n",
    "nlp = spacy.load(\"ca_core_news_sm\")\n",
    "doc = nlp(\"Setze jutges d'un jutjat menjaven fetges d'un penjat.\")\n",
    "print(\"Frase a analitzar: \"+doc.text+\"\\n\")\n",
    "for token in doc:\n",
    "    print(\"paraula: \"+token.text+\"\\t\",\"POS: \"+token.pos_+\"\\t\",\"lemma: \"+token.lemma_+\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
